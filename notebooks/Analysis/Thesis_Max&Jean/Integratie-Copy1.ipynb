{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --upgrade pip\n",
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import charts\n",
    "import inspect\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from opengrid.library import houseprint\n",
    "from scipy.signal import argrelextrema, butter, lfilter, freqz\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import norm\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 16,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hp = houseprint.Houseprint()\n",
    "#hp.sync_tmpos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for blockdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def znormalization(ts):\n",
    "    \"\"\"\n",
    "        This function ensures, that all elements are transformed into an output whose mean is approximately 0\n",
    "        while the standard deviation is in a range close to 1.\n",
    "    \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            ts: pandas dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    mus = ts.mean(axis = 0)\n",
    "    stds = ts.std(axis = 0)\n",
    "    \n",
    "    return (ts - mus) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paa_transform(ts, n_pieces):\n",
    "    \"\"\"\n",
    "        In order to reduce the dimensionality from n to M, we first divide the original time-series into M equally\n",
    "        sized frames and secondly compute the mean values for each frame. The sequence assembled from the mean\n",
    "        values is the PAA approximation (i.e., transform) of the original time-series.\n",
    "        As it was shown by Keogh et al, the complexity of the PAA transform can be reduced from O(NM) to O(Mm) \n",
    "        where m is the number of frames.\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            ts: numpy array\n",
    "            n_pieces: int\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    splitted = np.array_split(ts, n_pieces) ## along columns as we want\n",
    "    \n",
    "    return np.asarray(map(lambda xs: xs.mean(axis = 0), splitted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sax_transform(ts, n_pieces, alphabet=\"abcdefghijklmnop\"):\n",
    "    \"\"\"\n",
    "        In short, Symbolic Aggregate approXimation (SAX) algorithm application to the input time series transforms its into a strings.\n",
    "    \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            ts: numpy array\n",
    "            n_pieces: int\n",
    "            alphabet: string\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    alphabet_sz = len(alphabet)\n",
    "    thrholds = norm.ppf(np.linspace(1./alphabet_sz, \n",
    "                                    1-1./alphabet_sz, \n",
    "                                    alphabet_sz-1))\n",
    "    def translate(ts_values):\n",
    "        return np.asarray([(alphabet[0] if ts_value < thrholds[0]\n",
    "                else (alphabet[-1] if ts_value > thrholds[-1]\n",
    "                      else alphabet[np.where(thrholds <= ts_value)[0][-1]+1]))\n",
    "                           for ts_value in ts_values])\n",
    "    \n",
    "    paa_ts = paa_transform(znormalization(ts), n_pieces)\n",
    "    \n",
    "    return np.apply_along_axis(translate, 0, paa_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeArray(array1):\n",
    "    \"\"\"\n",
    "        turns string into array\n",
    "    \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            array1: pandas Dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas Dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    arrayValues=[]    \n",
    "    for i in range(0, array1.size-1):\n",
    "        if(i<array1.size):\n",
    "             arrayValues.append(ord(array1.iloc[i])-97)\n",
    "    df=pd.DataFrame(arrayValues)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_blocks(df_parent, row_parent, blocks, df_original, first_time = True):\n",
    "    \"\"\"\n",
    "        This function loops through the data and detects all possible events\n",
    "    \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            df_parent: pandas Dataframe\n",
    "            row_parent: int\n",
    "            blocks: pandas Dataframe\n",
    "            df_original: pandas Dataframe\n",
    "            first_time: Boolean\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas Dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    min_lvl = 0\n",
    "    start = False\n",
    "    \n",
    "    block_start = 0\n",
    "    block_end = 0\n",
    "    \n",
    "    nr_of_blocks_start = blocks.size\n",
    "    \n",
    "    nested = row_parent\n",
    "    \n",
    "    if first_time:\n",
    "        start = True\n",
    "        min_lvl = df_parent['repeatedValues'].min()\n",
    "        \n",
    "    #append last value again\n",
    "    post_index = df_parent.index[-1] + pd.Timedelta(minutes=1)\n",
    "\n",
    "    temp = pd.DataFrame(data=[df_parent.iloc[-1]], index=[post_index], columns=['repeatedValues'])\n",
    "    df_parent = df_parent.append(temp)\n",
    "\n",
    "    df_parent.sort_index(inplace=True)\n",
    "\n",
    "    for i in range(1, df_parent.size - 1):\n",
    "        if start == False and df_parent['repeatedValues'].iloc[i - 1] - df_parent['repeatedValues'].iloc[i] == 0:\n",
    "            min_lvl = df_parent['repeatedValues'].iloc[i]\n",
    "            start = True\n",
    "        \n",
    "        if start == True and df_parent['repeatedValues'].iloc[i] > min_lvl and df_parent['repeatedValues'].iloc[i - 1] == min_lvl:\n",
    "            block_start = df_parent.index[i - 1]\n",
    "            \n",
    "        if start == True and block_start != 0 and df_parent['repeatedValues'].iloc[i] <= min_lvl:\n",
    "            block_end = df_parent.index[i]\n",
    "            \n",
    "            #Match if the length is more than 10% less than the parent\n",
    "            temp = pd.DataFrame(data=[[str(block_start), str(block_end), nested]], columns=['start', 'stop', 'nested_in'])\n",
    "            \n",
    "            if float(df_parent.index.size - df_parent.ix[block_start : block_end].index.size) / float(df_parent.index.size) > 0.25:\n",
    "                blocks = blocks.append(temp, ignore_index=True)\n",
    "            \n",
    "            df_parent2 = df_original.ix[block_start : block_end]\n",
    "            row_parent = blocks.index.size - 1\n",
    "            blocks = find_blocks(df_parent2, row_parent, blocks, df_original, False)\n",
    "            \n",
    "            block_start = 0\n",
    "            block_end = 0\n",
    "            \n",
    "        if start == True and block_start == 0 and df_parent['repeatedValues'].iloc[i] < min_lvl:\n",
    "            #Lvl dropped below min value so the min value was not assigned properly\n",
    "            if df_parent['repeatedValues'].iloc[i+1] >= df_parent['repeatedValues'].iloc[i]:\n",
    "                \n",
    "                #Match if the length is more than 10% less than the parent\n",
    "                temp = pd.DataFrame(data=[[str(df_parent.index[0]), str(df_parent.index[i]), nested]], columns=['start', 'stop', 'nested_in'])\n",
    "                \n",
    "                if float(df_parent.index.size - df_parent.ix[df_parent.index[0] : df_parent.index[i]].index.size) / float(df_parent.index.size) > 0.10:\n",
    "                    blocks = blocks.append(temp, ignore_index=True)\n",
    "                \n",
    "                min_lvl = df_parent['repeatedValues'].iloc[i]\n",
    "                \n",
    "    \n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyValidationError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blockdetection(dataframe):\n",
    "    \"\"\"\n",
    "        This function prepares the data to be looped through\n",
    "    \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            dataframe: pandas Dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            df_rm: pandas Dataframe\n",
    "            blocks: pandas Dataframe\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    if dataframe.size <= 0:\n",
    "        raise MyValidationError(\"Dataframe must have a size bigger than 0\")\n",
    "    \n",
    "        return None, None\n",
    "    \n",
    "    #Take the rolling mean to filter out peaks\n",
    "    df_rm = dataframe.rolling(window=6,center=False).median()\n",
    "    df_rm.index = df_rm.index - pd.Timedelta(minutes=3)\n",
    "    df_rm = df_rm.dropna()\n",
    "    \n",
    "    #Z normalize\n",
    "    zScoresOriginalGraph=znormalization(df_rm)\n",
    "    \n",
    "    #Paa transform\n",
    "    splitData = paa_transform(zScoresOriginalGraph, df_rm.size/3) #size of df devided by 3 (if 3 in next line remains)\n",
    "    splitData_ext = np.repeat(splitData,3,axis = 0)\n",
    "    \n",
    "    #make sure the dataframes are of the same length to join the data and the index\n",
    "    shortage = df_rm.size - splitData_ext.size\n",
    "    if shortage > 0:\n",
    "        splitData_ext = np.append(splitData_ext, [splitData_ext[-1]]*shortage)\n",
    "\n",
    "    elif shortage < 0:\n",
    "        splitData_ext = splitData_ext[:abs(shortage)-1]\n",
    "\n",
    "    shortage = df_rm.size - splitData_ext.size\n",
    "    \n",
    "    df_paa = pd.DataFrame(index=df_rm.index, data=splitData_ext)\n",
    "    \n",
    "    #saxtransform\n",
    "    saxTransmation = sax_transform(df_paa, df_rm.size/3)\n",
    "    \n",
    "    #Revert the letters to numbers\n",
    "    letters=pd.DataFrame(saxTransmation)\n",
    "    letters.columns=['col1']\n",
    "    lettersToNumSax=makeArray(letters.col1) #Expects a pandaframe\n",
    "\n",
    "    repeatedValues=np.repeat(lettersToNumSax.as_matrix(),3) #expects an array\n",
    "    \n",
    "    #make sure the dataframes are of the same length to join the data and the index\n",
    "    shortage = df_rm.size - repeatedValues.size\n",
    "    if shortage > 0:\n",
    "        repeatedValues = np.append(repeatedValues, [repeatedValues[-1]]*shortage)\n",
    "\n",
    "    elif shortage < 0:\n",
    "        repeatedValues = repeatedValues[0:repeatedValues.size + shortage]\n",
    "    \n",
    "    shortage = df_rm.size - repeatedValues.size\n",
    "    \n",
    "    df_repeatedValues = pd.DataFrame(index=df_rm.index, data= repeatedValues, columns=['repeatedValues'])\n",
    "    \n",
    "    #pre- and append a zero to make sure the sample starts and ends with the same value\n",
    "    for i in range(0, 2):\n",
    "        pre_index = df_repeatedValues.index[0] - pd.Timedelta(minutes=1)\n",
    "        post_index = df_repeatedValues.index[-1] + pd.Timedelta(minutes=1)\n",
    "\n",
    "        temp = pd.DataFrame(data=[df_repeatedValues.min()], index=[pre_index], columns=['repeatedValues'])\n",
    "        df_repeatedValues = df_repeatedValues.append(temp)\n",
    "\n",
    "        temp = pd.DataFrame(data=[df_repeatedValues.min()], index=[post_index], columns=['repeatedValues'])\n",
    "        df_repeatedValues = df_repeatedValues.append(temp)\n",
    "\n",
    "        df_repeatedValues.sort_index(inplace=True)\n",
    "    \n",
    "    #Do the actual searching\n",
    "    blocks = pd.DataFrame(columns=['start', 'stop', 'nested_in'])\n",
    "    blocks = find_blocks(df_repeatedValues, -1, blocks, df_repeatedValues)\n",
    "    \n",
    "    #plot the dataframe with all the blocks\n",
    "    fig, ax = plt.subplots()\n",
    "    #ax.plot(df_repeatedValues.index, df_repeatedValues, color='grey')\n",
    "    ax.plot(df_rm.index, df_rm, color='grey')\n",
    "\n",
    "    colors = 100*['red', 'blue', 'orange', 'green', 'yellow']\n",
    "\n",
    "\n",
    "    for i in range(0, blocks.index.size):\n",
    "        #ax.axvspan(blocks['start'].iloc[i], blocks['stop'].iloc[i], alpha=0.1, color=colors[3])\n",
    "        ax.axvspan(str(pd.Timestamp(blocks['start'].iloc[i])), str(pd.Timestamp(blocks['stop'].iloc[i]) + pd.Timedelta(minutes=2)), alpha=0.1, color=colors[3])\n",
    "        #ax.axvspan(str(pd.Timestamp(blocks['start'].iloc[i])), str(pd.Timestamp(blocks['stop'].iloc[i])), alpha=0.1, color=colors[3])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    #return the rolling mean equivalent and the blocks dataframe\n",
    "    return df_rm, blocks#.drop_duplicates(subset=['start', 'stop'], keep= 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for pattern recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manhattanDistance(s1,s2):\n",
    "    \"\"\"\n",
    "        Calculate the manhattan similarity between the original array (s1) and it's possible match (s2)\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            s1: numpy array\n",
    "            s2: numpy array\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            float\n",
    "    \"\"\"\n",
    "    \n",
    "    dist = distance.cityblock(s1,s2)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_through_list(sensor_data, lookup_pattern):\n",
    "    \"\"\"\n",
    "        This function slides the desired pattern over the total data stream and detects the pieces that have an\n",
    "        acceptable similarity value (manhattanDistance)\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            sensor_data: pandas dataframe\n",
    "            lookup_pattern: pandas dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    start=time.time()\n",
    "    #Create list where values on which the different methods will act will be saved temporarily\n",
    "    #This list will be cleared every time\n",
    "    values_list=[]\n",
    "    #Create list with results of the methods\n",
    "    result_list=[]\n",
    "    compare_to_array=np.array(lookup_pattern)\n",
    "    i=0 #Loops through list arrayToCompare.size times\n",
    "    j=0 #Amount of values considered, and calculated.\n",
    "    array_with_values=np.array(sensor_data)\n",
    "    #start and stop timestamp\n",
    "    starting_time_stamp=sensor_data.index[0]\n",
    "    stop_time_stamp=sensor_data.index[lookup_pattern.size-1]\n",
    "    \n",
    "    #Threshold\n",
    "    threshold_value=lookup_pattern.size/10\n",
    "    \n",
    "    #LOOPING THROUGH LIST\n",
    "    while i < lookup_pattern.size+j: #i is dependent on j, i updates as j updates.\n",
    "        #Save x amount of values where x is the size of the array to compare.\n",
    "        values_list.append(([array_with_values[i][0]]))\n",
    "        i=i+1\n",
    "        \n",
    "        if(i==(lookup_pattern.size+j)) and (i<(sensor_data.size)):\n",
    "            \n",
    "            #j = the minute at this moment. i = starting from the current minute, adding the size of the array to compare.\n",
    "            #Stops running when the limit of values to be considered is reached, being the size the original array.\n",
    "            if(j==0) or (manhattanDistance(compare_to_array,values_list)!=result_list[len(result_list)-1][0]):\n",
    "                #Create list with unique values. Save them in \"resultList\". Euclidean score, startTimestamp, stopTimestamp\n",
    "                result_list.append([manhattanDistance(compare_to_array,values_list), starting_time_stamp,stop_time_stamp]) \n",
    "            \n",
    "            j=j+1\n",
    "            starting_time_stamp=sensor_data.index[j] #startingTimeStamp: current minute considered\n",
    "            stop_time_stamp=sensor_data.index[i] #stopTimeStamp: current minute considered + size of array to compare.\n",
    "            i=j\n",
    "            values_list=[]\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_minima_ts_correction(result_list):\n",
    "    \"\"\"\n",
    "        Checks if the selected 'best' matches overlap. In this case the match with the best similarity is kept\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            result_list: numpy array\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    start=time.time()\n",
    "    loop_index=0 #Loops through the resultslist, as long as its size\n",
    "    current_saves=0 #Current amount of variables saved\n",
    "    some_list=[]\n",
    "    df_result=pd.DataFrame(some_list)\n",
    "    ts=(result_list[0][2]-result_list[0][1])/10\n",
    "    threshold_value=(ts / np.timedelta64(1, 'm')).astype(int)\n",
    "    \n",
    "    #SAVING ONLY RELEVANT VALUES       \n",
    "    while loop_index < len(result_list):\n",
    "        \n",
    "        if(loop_index==0):\n",
    "            [value,start_time_stamp,stop_time_stamp] = [result_list[loop_index][0], result_list[loop_index][1], result_list[loop_index][2]]\n",
    "            df_result=df_result.append(pd.DataFrame([[value, start_time_stamp,stop_time_stamp]], index=[current_saves], columns=['Manhattan','startTimeStamp','stopTimeStamp']))\n",
    "            current_saves=current_saves+1\n",
    "            \n",
    "        if(result_list[loop_index-1][0] < result_list[loop_index-2][0]) and (result_list[loop_index-1][0] < result_list[loop_index][0]) and result_list[loop_index-1][0] < df_result.max()['Manhattan']:\n",
    "            #print \"Value\", resultList[loopIndex-1][0],\",Index:\",loopIndex-1,\"has a lower value than value left and right to it.(\",resultList[loopIndex-2][0],\",\",resultList[loopIndex][0],\") nIt will now replace\", dfResult['Euclidean'].max(),\"in the dataset.\"\n",
    "            [value,start_time_stamp,stop_time_stamp] = [result_list[loop_index-1][0], result_list[loop_index-1][1], result_list[loop_index-1][2]]\n",
    "            list_with_values=[]\n",
    "            i=0\n",
    "            max_value=0\n",
    "            \n",
    "            #!!!Timestamps are important. If timestamp is within the range of another timestamp already present,\n",
    "            #they will overwrite eachother instead of adding a new unique value\n",
    "            while i < df_result.index.size:\n",
    "                if(start_time_stamp >= df_result['startTimeStamp'][i]) and (start_time_stamp <= df_result['stopTimeStamp'][i]-pd.Timedelta(minutes=threshold_value)):\n",
    "                    \n",
    "                    #INSIDE BOUNDARIES\n",
    "                    if(df_result.loc[i][0] > max_value) and (df_result.loc[i][0]!=0): #Store the maximum value, of the range between start and stoptimestamp.\n",
    "                        max_value=df_result.loc[i][0]\n",
    "                        ts=df_result.loc[i][1]\n",
    "                    \n",
    "                    elif(df_result.loc[i][0]==0): #PERFECT MATCH\n",
    "                        max_value=1 #Not 0 because otherwise we assume it is outside of the boundaries\n",
    "                i=i+1\n",
    "                \n",
    "            if(max_value==0): \n",
    "                #OUTSIDE BOUNDARIES\n",
    "                    df_result=df_result.append(pd.DataFrame([[value, start_time_stamp,stop_time_stamp]], index=[current_saves], columns=['Manhattan','startTimeStamp','stopTimeStamp']))\n",
    "                    current_saves=current_saves+1\n",
    "                #INSIDE BOUNDARIES - \n",
    "                \n",
    "            elif(value < max_value): #Check if the current value is smaller than the Maximum value encountered. Replace if it is.\n",
    "                    df_result.loc[df_result['Manhattan']== max_value] = [value, start_time_stamp, stop_time_stamp]\n",
    "        \n",
    "        loop_index=loop_index+1\n",
    "    \n",
    "    df_result=df_result.sort_values(['Manhattan'])\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_threshold_value(df_result, nr_of_best_values, factor):\n",
    "    \"\"\"\n",
    "        Determine a broad threshold to filter out all manhattan local minima that are nothing like the desired frame\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            df_result: pandas dataframe\n",
    "            nr_of_best_values: int\n",
    "            factor: float\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_best_values=[]\n",
    "    i = 0\n",
    "    \n",
    "    if df_result.size >= nr_of_best_values:\n",
    "        for i in range(0, nr_of_best_values):\n",
    "            list_of_best_values.append(df_result.iloc[i])\n",
    "\n",
    "        list_of_best_values=pd.DataFrame(list_of_best_values)\n",
    "        mean=list_of_best_values.mean()\n",
    "        threshold=mean*factor\n",
    "\n",
    "        return df_result.loc[df_result['Manhattan']<=threshold[0]]\n",
    "    \n",
    "    else:\n",
    "        return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_split_nr(lookup_pattern):\n",
    "    \"\"\"\n",
    "        Find the number of peaks and determin the split number based on this result\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            lookup_pattern: pandas dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            int\n",
    "    \"\"\"\n",
    "    \n",
    "    cb = lookup_pattern.rolling(window=20).mean()['212ce724e124fbde0fb649396375d099'].as_matrix()\n",
    "    ind = argrelextrema(cb, np.greater)\n",
    "    \n",
    "    return len(ind) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(original, event_list, parts):\n",
    "    \"\"\"\n",
    "        Split the resulting matches in an amount of parts and store their means\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            original: pandas dataframe\n",
    "            event_list: pandas dataframe\n",
    "            parts: int\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = []\n",
    "    for i in range(0, parts):\n",
    "        cols.extend(['part_' + str(i)])    \n",
    "    parts_df = pd.DataFrame(columns=cols, index=event_list.index)\n",
    "    \n",
    "    for i in range(0, event_list['Manhattan'].size):\n",
    "        part_length = (event_list['stopTimeStamp'].iloc[i] - event_list['startTimeStamp'].iloc[i])/parts\n",
    "        \n",
    "        prev_end_time = event_list['startTimeStamp'].iloc[i]\n",
    "        for j in range(0, parts):\n",
    "            part_median = original.ix[prev_end_time: prev_end_time + part_length].mean().iloc[0]\n",
    "            prev_end_time = prev_end_time + part_length\n",
    "            index = event_list.index[i]\n",
    "            \n",
    "            parts_df.set_value(index, 'part_' + str(j), part_median)\n",
    "            \n",
    "    split_list = pd.concat([event_list, parts_df],axis=1)\n",
    "    \n",
    "    return split_list.sort_values('Manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster(matches, parts):\n",
    "    \"\"\"\n",
    "        Cluster best matches to label them as beeing a true match or not\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            matches: pandas dataframe\n",
    "            parts: int\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_data = [(matches['Manhattan']/matches['Manhattan'].max()).as_matrix()]\n",
    "    for i in range(0, parts):\n",
    "        cluster_data.extend([(matches['part_' + str(i)]/matches['part_' + str(i)].max()).as_matrix()])\n",
    "        \n",
    "    df_result = pd.DataFrame(data=cluster_data).transpose()\n",
    "    result=df_result.as_matrix()\n",
    "    \n",
    "    ms = MeanShift()\n",
    "    ms.fit(result)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "    n_cluster = len(np.unique(labels))\n",
    "    colors = 10*['g.', 'r.', 'c.', 'b.', 'k.', 'y.', 'm.']\n",
    "    \n",
    "    try:\n",
    "        del split_matches['Type']\n",
    "    except Exception as e:\n",
    "        #print 'No column named Type'\n",
    "        e = e\n",
    "    \n",
    "    \n",
    "    matches.insert(loc=0, column='Type', value=labels)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_matches(data, search_pattern):\n",
    "    \"\"\"\n",
    "        Run the total pattern recognition algorithm\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            data: pandas dataframe\n",
    "            search_pattern: pandas dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    all_minima = loop_through_list(data, search_pattern)\n",
    "    selected_minima = local_minima_ts_correction(all_minima)\n",
    "    \n",
    "    best_matches = calculate_threshold_value(selected_minima, 20, 2)\n",
    "    \n",
    "    split_number = determine_split_nr(search_pattern)\n",
    "    split_matches = split(data, best_matches, split_number)\n",
    "    \n",
    "    split_matches = cluster(split_matches, split_number)\n",
    "    \n",
    "    return split_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_matches(split_matches, match):\n",
    "    \"\"\"\n",
    "        Function to either plot all matches or plot all non matches\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            split_matches: pandas dataframe\n",
    "            match: bool\n",
    "    \"\"\"\n",
    "    \n",
    "    if match:\n",
    "        for i in range(0, split_matches.index.size):\n",
    "            if split_matches['Type'].iloc[i] == split_matches['Type'].iloc[0] or split_matches['Type'].iloc[i] == split_matches['Type'].iloc[1]:\n",
    "                plt.plot(data.ix[pd.Timestamp(split_matches['startTimeStamp'].iloc[i]) - pd.Timedelta(minutes=2):pd.Timestamp(split_matches['stopTimeStamp'].iloc[i]) + pd.Timedelta(minutes=2)])     \n",
    "    \n",
    "    else:\n",
    "        for i in range(0, split_matches.index.size):\n",
    "            if split_matches['Type'].iloc[i] != split_matches['Type'].iloc[0] and split_matches['Type'].iloc[i] != split_matches['Type'].iloc[1]:\n",
    "                plt.plot(data.ix[pd.Timestamp(split_matches['startTimeStamp'].iloc[i]) - pd.Timedelta(minutes=2):pd.Timestamp(split_matches['stopTimeStamp'].iloc[i]) + pd.Timedelta(minutes=2)])     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to combine blockdetection and pattern recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sensor\n",
    "gas = hp.find_sensor('212ce724e124fbde0fb649396375d099')\n",
    "\n",
    "#Data\n",
    "head = pd.Timestamp('2015-11-17 01:00:00')\n",
    "tail = pd.Timestamp('2015-11-18 01:00:00')\n",
    "data = gas.get_data(head=head, tail=tail, diff=True, resample='min', unit='kW')\n",
    "data=pd.DataFrame(data)\n",
    "data = data.dropna()\n",
    "\n",
    "#Plot the data\n",
    "charts.plot(data, stock=True, show='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    original_rm, blocks = blockdetection(data)\n",
    "except MyValidationError as exception:\n",
    "    # handle exception here and get error message\n",
    "    print exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern recognition using DTW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def determine_split_nr(lookup_pattern):\n",
    "    \"\"\"\n",
    "        Find the number of peaks and determin the split number based on this result\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            lookup_pattern: pandas dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            int\n",
    "    \"\"\"\n",
    "    \n",
    "    lookup_pattern = pd.DataFrame(data=lookup_pattern)\n",
    "    lookup_pattern.columns = ['pattern']\n",
    "    \n",
    "    cb = lookup_pattern.rolling(window=20).mean()['pattern'].as_matrix()\n",
    "    ind = argrelextrema(cb, np.greater)\n",
    "    \n",
    "    return len(ind) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split(original, event_list, parts):\n",
    "    \"\"\"\n",
    "        Split the resulting matches in an amount of parts and store their means\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            original: pandas dataframe\n",
    "            event_list: pandas dataframe\n",
    "            parts: int\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = []\n",
    "    for i in range(0, parts):\n",
    "        cols.extend(['part_' + str(i)])    \n",
    "    parts_df = pd.DataFrame(columns=cols, index=event_list.index)\n",
    "    \n",
    "    for i in range(0, event_list.index.size):\n",
    "        part_length = (pd.Timestamp(event_list['stop'].iloc[i]) - pd.Timestamp(event_list['start'].iloc[i]))/parts\n",
    "        \n",
    "        prev_end_time = pd.Timestamp(event_list['start'].iloc[i])\n",
    "        for j in range(0, parts):\n",
    "            part_median = original.ix[prev_end_time: prev_end_time + part_length].mean().iloc[0]\n",
    "            prev_end_time = prev_end_time + part_length\n",
    "            index = event_list.index[i]\n",
    "            \n",
    "            parts_df.set_value(index, 'part_' + str(j), part_median)\n",
    "            \n",
    "    split_list = pd.concat([event_list, parts_df],axis=1)\n",
    "    \n",
    "    return split_list.sort_values('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster(matches, parts):\n",
    "    \"\"\"\n",
    "        Cluster best matches to label them as beeing a true match or not\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            matches: pandas dataframe\n",
    "            parts: int\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    if matches['cost'].max() == 0:\n",
    "        max_cost = 1\n",
    "    else:\n",
    "        max_cost = matches['cost'].max()\n",
    "    \n",
    "    cluster_data = [(matches['cost']/max_cost).as_matrix()]\n",
    "    for i in range(0, parts):\n",
    "        cluster_data.extend([(matches['part_' + str(i)]/matches['part_' + str(i)].max()).as_matrix()])\n",
    "        \n",
    "    df_result = pd.DataFrame(data=cluster_data).transpose()\n",
    "    result=df_result.as_matrix()\n",
    "    \n",
    "    ms = MeanShift()\n",
    "    ms.fit(result)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "    n_cluster = len(np.unique(labels))\n",
    "    colors = 10*['g.', 'r.', 'c.', 'b.', 'k.', 'y.', 'm.']\n",
    "    \n",
    "    try:\n",
    "        del split_matches['Type']\n",
    "    except Exception as e:\n",
    "        #print 'No column named Type'\n",
    "        e = e\n",
    "    \n",
    "    matches.insert(loc=0, column='Type', value=labels)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_matches(split_matches, match):\n",
    "    \"\"\"\n",
    "        Function to either plot all matches or plot all non matches\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            split_matches: pandas dataframe\n",
    "            match: bool\n",
    "    \"\"\"\n",
    "    \n",
    "    if match:\n",
    "        for i in range(0, split_matches.index.size):\n",
    "            if split_matches['Type'].iloc[i] == split_matches['Type'].iloc[0] or split_matches['Type'].iloc[i] == split_matches['Type'].iloc[1]:\n",
    "                plt.plot(data.ix[pd.Timestamp(split_matches['start'].iloc[i]) - pd.Timedelta(minutes=2):pd.Timestamp(split_matches['stop'].iloc[i]) + pd.Timedelta(minutes=2)])     \n",
    "    \n",
    "    else:\n",
    "        for i in range(0, split_matches.index.size):\n",
    "            if split_matches['Type'].iloc[i] != split_matches['Type'].iloc[0] and split_matches['Type'].iloc[i] != split_matches['Type'].iloc[1]:\n",
    "                plt.plot(data.ix[pd.Timestamp(split_matches['start'].iloc[i]) - pd.Timedelta(minutes=2):pd.Timestamp(split_matches['stop'].iloc[i]) + pd.Timedelta(minutes=2)])     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_threshold_value(df_result, nr_of_best_values, factor):\n",
    "    \"\"\"\n",
    "        Determine a broad threshold to filter out all manhattan local minima that are nothing like the desired frame\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            df_result: pandas dataframe\n",
    "            nr_of_best_values: int\n",
    "            factor: float\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_best_values=[]\n",
    "    i = 0\n",
    "    \n",
    "    if df_result.index.size > nr_of_best_values:\n",
    "        for i in range(0, nr_of_best_values):\n",
    "            list_of_best_values.append(df_result.iloc[i])\n",
    "\n",
    "        list_of_best_values=pd.DataFrame(list_of_best_values)\n",
    "        mean=list_of_best_values.mean()\n",
    "        threshold=mean*factor\n",
    "\n",
    "        return df_result.loc[df_result['cost']<=threshold[0]]\n",
    "    \n",
    "    else:\n",
    "        return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DTW_distance(sample_1, sample_2):\n",
    "    \"\"\"\n",
    "        Calculate the similarity between the two samples according to the diferential time warping method\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            sample_1: pandas dataframe\n",
    "            sample_2: pandas dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            float\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_1 = sample_1.as_matrix()\n",
    "    sample_2 = sample_2.as_matrix()\n",
    "    \n",
    "    #compare sample_1 and sample_2 for similarity\n",
    "    #Get distance\n",
    "    distances = np.zeros((len(sample_1), len(sample_2)))\n",
    "    \n",
    "    for i in range(len(sample_1)):\n",
    "        for j in range(len(sample_2)):\n",
    "            #distances[i,j] = (sample_2[j]-sample_1[i])**2\n",
    "            distances[i,j] = abs(sample_2[j]-sample_1[i]) \n",
    "\n",
    "\n",
    "    #get accumulated distance\n",
    "    accumulated_cost = np.zeros((len(sample_1), len(sample_2)))\n",
    "    accumulated_cost[0,0] = distances[0,0]\n",
    "    \n",
    "    for i in range(0, len(sample_1)):\n",
    "        for j in range(0, len(sample_2)):\n",
    "            if i==0 and j==0:\n",
    "                #i!=0 and j!=0 didn't work..\n",
    "                i = i\n",
    "            else:\n",
    "                if i == 0:\n",
    "                    accumulated_cost[i, j] = accumulated_cost[i, j-1] + distances[i, j]\n",
    "                elif j == 0:\n",
    "                    accumulated_cost[i, j] = accumulated_cost[i-1, j] + distances[i, j]\n",
    "                else:\n",
    "                    accumulated_cost[i, j] = min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]) + distances[i, j]\n",
    "                    \n",
    "\n",
    "    path = [[len(sample_2)-1, len(sample_1)-1]]\n",
    "    cost = 0\n",
    "    \n",
    "    i = len(sample_1)-1\n",
    "    j = len(sample_2)-1\n",
    "    \n",
    "    while i>0 and j>0:\n",
    "        if i==0:\n",
    "            j = j - 1\n",
    "        elif j==0:\n",
    "            i = i - 1\n",
    "        else:\n",
    "            if accumulated_cost[i-1, j] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
    "                i = i - 1\n",
    "            elif accumulated_cost[i, j-1] == min(accumulated_cost[i-1, j-1], accumulated_cost[i-1, j], accumulated_cost[i, j-1]):\n",
    "                j = j-1\n",
    "            else:\n",
    "                i = i - 1\n",
    "                j= j- 1\n",
    "        path.append([j, i])\n",
    "\n",
    "    for [y, x] in path:\n",
    "        cost = cost + distances[x, y]\n",
    "\n",
    "    #Plot Graphs and dwt distance\n",
    "    #path_ts_2 = [point[0] for point in path]\n",
    "    #path_ts_1 = [point[1] for point in path]\n",
    "\n",
    "    #plt.figure()\n",
    "\n",
    "    #plt.subplot(121)\n",
    "    #plt.plot(sample_1)\n",
    "    #plt.plot(sample_2)\n",
    "\n",
    "    #plt.subplot(122)\n",
    "    #distance_cost_plot(accumulated_cost)\n",
    "    #plt.plot(path_ts_2, path_ts_1);\n",
    "    #plt.show()\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_if_child(blocks, sample_1_id, sample_2_id):\n",
    "    parent_1 = sample_1_id\n",
    "    parent_2 = sample_2_id\n",
    "    \n",
    "    do_check = False\n",
    "    \n",
    "    found = False\n",
    "    while not found:\n",
    "        #If they are \n",
    "        if parent_1 != -1 and parent_1 != sample_2_id:\n",
    "            parent_1 = blocks['nested_in'].loc[parent_1]\n",
    "            \n",
    "        elif parent_1 == sample_2_id:\n",
    "            do_check = False\n",
    "            found = True\n",
    "        \n",
    "        elif parent_1 == -1:\n",
    "            do_check = True\n",
    "            found = True\n",
    "    \n",
    "    if do_check:\n",
    "        found = False\n",
    "        while not found:\n",
    "            #If they are \n",
    "            if parent_2 != -1 and parent_2 != sample_1_id:\n",
    "                parent_2 = blocks['nested_in'].loc[parent_2]\n",
    "\n",
    "            elif parent_2 == sample_1_id:\n",
    "                do_check = False\n",
    "                found = True\n",
    "\n",
    "            elif parent_2 == -1:\n",
    "                do_check = True\n",
    "                found = True\n",
    "        \n",
    "    return do_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_overlap(search_pattern, found_patterns):\n",
    "    \"\"\"\n",
    "        Check if the lookup pattern has already been found\n",
    "        \n",
    "        PARAMETERS\n",
    "        ----------\n",
    "            sample_1: pandas dataframe\n",
    "            sample_2: pandas dataframe\n",
    "            \n",
    "        RETURNS\n",
    "        -------\n",
    "            Boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    search_start = search_pattern.index[0]\n",
    "    search_stop = search_pattern.index[-1]\n",
    "    \n",
    "    for i in range(0, found_patterns.index.size):\n",
    "        found_start = pd.Timestamp(found_patterns['start'].iloc[i])\n",
    "        found_stop = pd.Timestamp(found_patterns['stop'].iloc[i])\n",
    "        \n",
    "        if search_start > found_start and search_start < found_stop and search_stop > found_stop:\n",
    "            overlap = 1 - ((search_start - found_start)/(found_stop - found_start))\n",
    "            if overlap > 0.5:\n",
    "                return False\n",
    "        \n",
    "        elif search_stop > found_start and search_stop < found_stop and search_start < found_start:\n",
    "            overlap = ((search_stop - found_start)/(found_stop - found_start))\n",
    "            if overlap > 0.5:\n",
    "                return False\n",
    "        \n",
    "        elif search_start > found_start and search_start < found_stop and search_stop > found_start and search_stop < found_stop:\n",
    "            return False\n",
    "        \n",
    "        elif search_start < found_start and search_start > found_stop and search_stop < found_start and search_stop > found_stop:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del blocks['Type']\n",
    "except Exception as e:\n",
    "    print 'No column named Type'\n",
    "\n",
    "zeros = np.zeros(blocks.index.size)\n",
    "blocks.insert(loc=0, column='Type', value=zeros)\n",
    "\n",
    "types = pd.DataFrame(columns=['start', 'stop', 'Type'])\n",
    "\n",
    "event_type = 1\n",
    "for i in range(0, blocks.index.size):\n",
    "    sys.stdout.write(\"\\r\" + str(i) + '/' + str(blocks.index.size))\n",
    "    \n",
    "    dtw_cost = pd.DataFrame(columns=['start', 'stop', 'cost'])\n",
    "    \n",
    "    #If the event has not been recognized yet check with other events\n",
    "    if blocks['Type'].iloc[i] == 0:\n",
    "    \n",
    "        search_pattern = pd.DataFrame(data=original_rm).ix[pd.Timestamp(blocks['start'].iloc[i]) - pd.Timedelta(minutes=2):pd.Timestamp(blocks['stop'].iloc[i]) + pd.Timedelta(minutes=2)]    \n",
    "        temp = pd.DataFrame(data=[[blocks['start'].iloc[i], blocks['stop'].iloc[i], 0]], index=[blocks.index[i]], columns=['start', 'stop', 'cost'])\n",
    "        dtw_cost = dtw_cost.append(temp)\n",
    "        \n",
    "        for j in range(i, blocks.index.size):\n",
    "            \n",
    "            if blocks['nested_in'].iloc[j] == -1:\n",
    "                compare_pattern = pd.DataFrame(data=original_rm).ix[pd.Timestamp(blocks['start'].iloc[j]) - pd.Timedelta(minutes=2):pd.Timestamp(blocks['stop'].iloc[j]) + pd.Timedelta(minutes=2)]    \n",
    "\n",
    "                #Check if the length is more or less the same\n",
    "                if search_pattern.size < compare_pattern.size * 1.25 and search_pattern.size > compare_pattern.size * 0.75:\n",
    "                    #Check if the blocks are nested in each other\n",
    "                    if check_if_child(blocks, blocks.index[i], blocks.index[j]):\n",
    "                        #Check if the pattern has already been found\n",
    "                        if (check_overlap(search_pattern, types)):\n",
    "\n",
    "                            #Manthattan\n",
    "                            if (search_pattern.size < compare_pattern.size):\n",
    "                                res = loop_through_list(compare_pattern, search_pattern)\n",
    "                            else:\n",
    "                                res = loop_through_list(search_pattern, compare_pattern)\n",
    "\n",
    "                            if len(res) > 0:\n",
    "                                temp = pd.DataFrame(data=res, columns=['cost', 'start', 'stop']).iloc[0]\n",
    "\n",
    "                                #store all costs and id's\n",
    "                                #temp = pd.DataFrame(data=[[blocks['start'].iloc[j], blocks['stop'].iloc[j], cost]], index=[blocks.index[j]], columns=['start', 'stop', 'cost'])\n",
    "                                dtw_cost = dtw_cost.append(temp)\n",
    "        \n",
    "        if dtw_cost.index.size > 3:\n",
    "            \n",
    "            dtw_cost = calculate_threshold_value(dtw_cost, 20, 2)\n",
    "        \n",
    "            split_number = determine_split_nr(search_pattern)\n",
    "            split_matches = split(original_rm, dtw_cost, split_number)\n",
    "        \n",
    "            split_matches = cluster(split_matches, split_number)\n",
    "        \n",
    "            #all events labled the same as row 0 or 1 are a match\n",
    "            lable_1 = split_matches['Type'].iloc[0] \n",
    "            lable_2 = split_matches['Type'].iloc[1]\n",
    "        \n",
    "            for j in range(0, split_matches.index.size):\n",
    "                if split_matches['Type'].iloc[j] == lable_1 or split_matches['Type'].iloc[j] == lable_2:\n",
    "                    types = types.append(pd.DataFrame(data=[[split_matches['start'].iloc[j], split_matches['stop'].iloc[j], event_type]], columns=['start', 'stop', 'Type']))\n",
    "        \n",
    "            event_type = event_type + 1\n",
    "        \n",
    "        del dtw_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for j in range(0, 100):\n",
    "    plt.figure(j)\n",
    "    count = 0\n",
    "    for i in range(0, blocks.index.size):\n",
    "        if blocks['nested_in'].iloc[i] == j:\n",
    "            plt.plot(original_rm.ix[pd.Timestamp(blocks['start'].iloc[i]) - pd.Timedelta(minutes=2):pd.Timestamp(blocks['stop'].iloc[i]) + pd.Timedelta(minutes=2)])\n",
    "            count = count + 1\n",
    "    \n",
    "    if(count > 0):\n",
    "        plt.plot(original_rm.ix[pd.Timestamp(blocks['start'].loc[j]) - pd.Timedelta(minutes=2):pd.Timestamp(blocks['stop'].loc[j]) + pd.Timedelta(minutes=2)])\n",
    "            \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "plt.plot(original_rm.ix[pd.Timestamp(blocks['start'].iloc[i]) :pd.Timestamp(blocks['stop'].iloc[i])])\n",
    "i = 9\n",
    "plt.plot(original_rm.ix[pd.Timestamp(blocks['start'].iloc[i]) :pd.Timestamp(blocks['stop'].iloc[i])])\n",
    "i = 5\n",
    "plt.plot(original_rm.ix[pd.Timestamp(blocks['start'].iloc[i]) :pd.Timestamp(blocks['stop'].iloc[i])])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}